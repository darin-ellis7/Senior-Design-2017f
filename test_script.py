# libraries that need to be downloaded (preferably using pip)
from feedparser import *
from newspaper import *
import requests
import tldextract

# libraries that should be built into Python (don't need to be downloaded)
from urllib import parse as urlparse
import re
import html
import ssl

# I have no idea what this is or what it does but it makes this script work on Python 3.6
if hasattr(ssl, '_create_unverified_context'):
    ssl._create_default_https_context = ssl._create_unverified_context

# this is a test version of the script (doesn't do any database addition)
# we can use this to help gauge whether our relevancy check is working

# Links in the RSS feed are generated by google, and the actual URL of the article is stored as the 'url' parameter in this link
# this function gets us the actual URL
def getURL(RSS_url):
    parsed = urlparse.urlparse(RSS_url)
    url = urlparse.parse_qs(parsed.query)['url'][0]
    return url

# 'Supreme Court' appears in the titles of the RSS feed with bold tags around them
# this function strips the HTML and returns a text-only version of the title
def cleanTitle(original_title):
    cleanr = re.compile('<.*?>')
    cleanTitle = html.unescape(re.sub(cleanr, '', original_title))
    return cleanTitle

# original date is a long date/time string
# for our purposes, we really only need date, not time - so this function extracts the date and converts it into a month/date/year format
def convertDate(orig_date):
    convertedDate = datetime.datetime.strptime(orig_date,"%Y-%m-%dT%H:%M:%SZ").strftime('%Y-%m-%d')
    return convertedDate

# parses URL to get the domain name of each article's link - the source
# one defect in handling the source is that, as of now, we don't know how to handle multiple-word sources beyond just storing it all as one string (so Fox News would just be stored as foxnews)
def getSource(url):
    ext = tldextract.extract(url)
    source = ext.domain
    return source

# Newspaper library can get grab keywords from articles in conjunction with the nltk (natural language toolkit) library
# this function prepares the article for language processing and returns an array of keywords from the article
def getKeywords(article):
    article.nlp()
    return article.keywords


# barebones check for relevancy because local and international news are starting to creep into the database
# seems to get pretty good results (it's quite selective)
def relevant(keywords, title,source):
    title = title.lower()
    
    avoidedSources = ['indiatimes','thehindu']
    
    foreignCountries = ['india','kenya','canada']
    
    states = ["Alabama","Alaska","Arizona","Arkansas","California","Colorado",
  "Connecticut","Delaware","Florida","Georgia","Hawaii","Idaho","Illinois",
  "Indiana","Iowa","Kansas","Kentucky","Louisiana","Maine","Maryland",
  "Massachusetts","Michigan","Minnesota","Mississippi","Missouri","Montana",
  "Nebraska","Nevada","New Hampshire","New Jersey","New Mexico","New York",
  "North Carolina","North Dakota","Ohio","Oklahoma","Oregon","Pennsylvania",
  "Rhode Island","South Carolina","South Dakota","Tennessee","Texas","Utah",
  "Vermont","Virginia","Washington","West Virginia","Wisconsin","Wyoming"]
    
    stateAbbreviations = ["AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DC", "DE", "FL", "GA", 
          "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD", 
          "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", 
          "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", 
          "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY"]
    
    if 'us supreme court' in title or 'u.s. supreme court' in title:
        return True
    
    if 'supreme' not in keywords or 'court' not in keywords:
        return False
    
    if source.lower() in avoidedSources:
        return False
    
    for country in foreignCountries:
        if country in title or country in keywords:
            return False
    
    if 'supreme court' in title:
        if 'state supreme court' in title:
            return False
        
        for state in states:
            if state.lower() + ' supreme court' in title:
                return False
        
        for abv in stateAbbreviations:
            if abv.lower() + ' supreme court' in title:
                split = title.split()
                if abv.lower() in split:
                    return False
    
    return True
                 
# goes through each entry in a given feed, check it for relevancy, and if relevant, add it to the database
# if we can't get the data from the website (403/404 errors, whatever) - an exception occurs, and we move to the next article
def parseFeed(RSS):
    # config info for Newspaper - keep article html for user-friendly display, set browser user agent to a desktop computer header to help fight 403 errors
    config = Config()
    config.keep_article_html = False
    config.browser_user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'

    # begin parsing
    feed = parse(RSS)
    total = len(feed.entries)
    successes = 0
    for post in feed.entries:
        url = getURL(post['link'])
        title = cleanTitle(post['title'])
        date = convertDate(post['date'])
        source = getSource(url)
        
    
        print('url:',url)
        print('title:',title)
        print('date:',date)
        print('source:',source)
        
        # we'll do relevancy check sometime around here
        a = Article(url,config)
        try:
            a.download()
            a.parse()
            
            # since tiny articles are generally useless, we check for length here
            # this also helps us weed out paywalls, snippets, maybe even some local news sources
            text = a.text
            if len(text) > 500:
                if(len(a.authors) > 0):
                    author = a.authors[0]
                else:
                    author = 'Unknown'
                    
                keywords = getKeywords(a)
                if a.top_image == '':
                    image = None
                else:
                    image = a.top_image
                print('author:',author)
                print('image',image)
                print(keywords)
                
                if not relevant(keywords,title,source):
                    print('Rejected - Deemed irrelevant')
                else:
                    successes += 1
                    print('Added')
            
            else:
                print('Rejected - too short')
            
        except ArticleException:
            print('Rejected - error occurred')
        print()
    print(successes,"/",total,"articles added to database.")
    print('=======================================================')

def main():
    
    #Google Alert custom feeds
    feeds = ['https://www.google.com/alerts/feeds/16346142240605984801/8005087395970124365','https://www.google.com/alerts/feeds/16346142240605984801/12974548777403563412','https://www.google.com/alerts/feeds/16346142240605984801/10736272716109264625','https://www.google.com/alerts/feeds/16346142240605984801/11622717956471844582']
    for feed in feeds:
        parseFeed(feed)

main()
